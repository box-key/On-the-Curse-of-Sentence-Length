{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utilities import GRU\n",
    "\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "model = GRU.Seq2Seq(input_dim=INPUT_DIM, \n",
    "                      enc_emb_dim=ENC_EMB_DIM, \n",
    "                      hid_dim=HID_DIM, \n",
    "                      enc_dropout=ENC_DROPOUT,\n",
    "                      output_dim=OUTPUT_DIM, \n",
    "                      dec_emb_dim=DEC_EMB_DIM,\n",
    "                      dec_dropout=DEC_DROPOUT,\n",
    "                      device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): GRU(768, 512)\n",
       "    (fc_out): Linear(in_features=1280, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utilities.TrainModel import TrainModel\n",
    "\n",
    "train_GRU = TrainModel(model=model,\n",
    "                         train_iterator=train_iterator,\n",
    "                         val_iterator=valid_iterator,\n",
    "                         optimizer=optimizer,\n",
    "                         criterion=criterion,\n",
    "                         model_type='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 37s\n",
      "\tTrain Loss: 5.036 | Train PPL: 153.924\n",
      "\t Val. Loss: 5.335 |  Val. PPL: 207.373\n",
      "Epoch: 02 | Time: 0m 37s\n",
      "\tTrain Loss: 4.379 | Train PPL:  79.759\n",
      "\t Val. Loss: 5.130 |  Val. PPL: 169.084\n",
      "Epoch: 03 | Time: 0m 37s\n",
      "\tTrain Loss: 4.086 | Train PPL:  59.488\n",
      "\t Val. Loss: 4.786 |  Val. PPL: 119.875\n",
      "Epoch: 04 | Time: 0m 37s\n",
      "\tTrain Loss: 3.777 | Train PPL:  43.675\n",
      "\t Val. Loss: 4.405 |  Val. PPL:  81.846\n",
      "Epoch: 05 | Time: 0m 37s\n",
      "\tTrain Loss: 3.490 | Train PPL:  32.777\n",
      "\t Val. Loss: 4.246 |  Val. PPL:  69.842\n",
      "Epoch: 06 | Time: 0m 38s\n",
      "\tTrain Loss: 3.238 | Train PPL:  25.493\n",
      "\t Val. Loss: 4.068 |  Val. PPL:  58.461\n",
      "Epoch: 07 | Time: 0m 38s\n",
      "\tTrain Loss: 2.960 | Train PPL:  19.295\n",
      "\t Val. Loss: 3.857 |  Val. PPL:  47.344\n",
      "Epoch: 08 | Time: 0m 37s\n",
      "\tTrain Loss: 2.747 | Train PPL:  15.602\n",
      "\t Val. Loss: 3.765 |  Val. PPL:  43.160\n",
      "Epoch: 09 | Time: 0m 37s\n",
      "\tTrain Loss: 2.525 | Train PPL:  12.493\n",
      "\t Val. Loss: 3.758 |  Val. PPL:  42.843\n",
      "Epoch: 10 | Time: 0m 38s\n",
      "\tTrain Loss: 2.330 | Train PPL:  10.282\n",
      "\t Val. Loss: 3.699 |  Val. PPL:  40.415\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "train_GRU.epoch(n_epochs=N_EPOCHS, clip=CLIP,model_name='att-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            include_lengths = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     sort_within_batch = True,\n",
    "     sort_key = lambda x : len(x.src),\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utilities import BiGRUwithAttention\n",
    "\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "\n",
    "model2 = BiGRUwithAttention.Seq2Seq(input_dim=INPUT_DIM, \n",
    "                                  enc_emb_dim=ENC_EMB_DIM, \n",
    "                                  enc_hid_dim=ENC_HID_DIM,\n",
    "                                  enc_dropout=ENC_DROPOUT,\n",
    "                                  output_dim=OUTPUT_DIM, \n",
    "                                  dec_emb_dim=DEC_EMB_DIM,\n",
    "                                  dec_hid_dim=DEC_HID_DIM,\n",
    "                                  dec_dropout=DEC_DROPOUT,\n",
    "                                  src_pad_idx=SRC_PAD_IDX,\n",
    "                                  device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model2.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model2.parameters())\n",
    "\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Epoch: 01 | Time: 0m 45s\n",
      "\tTrain Loss: 5.008 | Train PPL: 149.661\n",
      "\t Val. Loss: 4.730 |  Val. PPL: 113.332\n",
      "Epoch: 02 | Time: 0m 45s\n",
      "\tTrain Loss: 3.974 | Train PPL:  53.187\n",
      "\t Val. Loss: 4.039 |  Val. PPL:  56.782\n",
      "Epoch: 03 | Time: 0m 45s\n",
      "\tTrain Loss: 3.301 | Train PPL:  27.149\n",
      "\t Val. Loss: 3.588 |  Val. PPL:  36.159\n",
      "Epoch: 04 | Time: 0m 46s\n",
      "\tTrain Loss: 2.805 | Train PPL:  16.533\n",
      "\t Val. Loss: 3.381 |  Val. PPL:  29.400\n",
      "Epoch: 05 | Time: 0m 46s\n",
      "\tTrain Loss: 2.435 | Train PPL:  11.413\n",
      "\t Val. Loss: 3.309 |  Val. PPL:  27.370\n",
      "Epoch: 06 | Time: 0m 46s\n",
      "\tTrain Loss: 2.155 | Train PPL:   8.626\n",
      "\t Val. Loss: 3.301 |  Val. PPL:  27.144\n",
      "Epoch: 07 | Time: 0m 45s\n",
      "\tTrain Loss: 1.927 | Train PPL:   6.869\n",
      "\t Val. Loss: 3.206 |  Val. PPL:  24.687\n",
      "Epoch: 08 | Time: 0m 45s\n",
      "\tTrain Loss: 1.747 | Train PPL:   5.737\n",
      "\t Val. Loss: 3.298 |  Val. PPL:  27.070\n",
      "Epoch: 09 | Time: 0m 45s\n",
      "\tTrain Loss: 1.566 | Train PPL:   4.790\n",
      "\t Val. Loss: 3.346 |  Val. PPL:  28.381\n",
      "Epoch: 10 | Time: 0m 45s\n",
      "\tTrain Loss: 1.442 | Train PPL:   4.230\n",
      "\t Val. Loss: 3.348 |  Val. PPL:  28.457\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utilities.TrainModel import TrainModel\n",
    "\n",
    "train_AttentionGRU = TrainModel(model=model2,\n",
    "                         train_iterator=train_iterator,\n",
    "                         val_iterator=valid_iterator,\n",
    "                         optimizer=optimizer,\n",
    "                         criterion=criterion,\n",
    "                         model_type='Attention')\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "train_AttentionGRU.epoch(n_epochs=N_EPOCHS, clip=CLIP,model_name='att-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))\n",
    "\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utilities import Transformer\n",
    "\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_HID_DIM = 256\n",
    "DEC_HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model3 = Transformer.Seq2Seq(input_dim=INPUT_DIM, \n",
    "                                  enc_hid_dim=ENC_HID_DIM,\n",
    "                                  enc_layers=ENC_LAYERS,\n",
    "                                  enc_heads=ENC_HEADS,\n",
    "                                  enc_pf_dim=ENC_PF_DIM,\n",
    "                                  enc_dropout=ENC_DROPOUT,\n",
    "                                  output_dim=OUTPUT_DIM,\n",
    "                                  dec_hid_dim=DEC_HID_DIM,\n",
    "                                  dec_layers=DEC_LAYERS,\n",
    "                                  dec_heads=DEC_HEADS,\n",
    "                                  dec_pf_dim=DEC_PF_DIM,\n",
    "                                  dec_dropout=DEC_DROPOUT,\n",
    "                                  src_pad_idx=SRC_PAD_IDX,\n",
    "                                  trg_pad_idx=TRG_PAD_IDX, \n",
    "                                  device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model3.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model3.parameters())\n",
    "\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Epoch: 01 | Time: 0m 16s\n",
      "\tTrain Loss: 5.552 | Train PPL: 257.858\n",
      "\t Val. Loss: 3.938 |  Val. PPL:  51.311\n",
      "Epoch: 02 | Time: 0m 17s\n",
      "\tTrain Loss: 3.720 | Train PPL:  41.244\n",
      "\t Val. Loss: 3.369 |  Val. PPL:  29.057\n",
      "Epoch: 03 | Time: 0m 17s\n",
      "\tTrain Loss: 3.275 | Train PPL:  26.452\n",
      "\t Val. Loss: 3.036 |  Val. PPL:  20.821\n",
      "Epoch: 04 | Time: 0m 17s\n",
      "\tTrain Loss: 2.951 | Train PPL:  19.116\n",
      "\t Val. Loss: 2.781 |  Val. PPL:  16.138\n",
      "Epoch: 05 | Time: 0m 17s\n",
      "\tTrain Loss: 2.669 | Train PPL:  14.425\n",
      "\t Val. Loss: 2.544 |  Val. PPL:  12.727\n",
      "Epoch: 06 | Time: 0m 17s\n",
      "\tTrain Loss: 2.387 | Train PPL:  10.877\n",
      "\t Val. Loss: 2.303 |  Val. PPL:  10.008\n",
      "Epoch: 07 | Time: 0m 17s\n",
      "\tTrain Loss: 2.101 | Train PPL:   8.178\n",
      "\t Val. Loss: 2.107 |  Val. PPL:   8.224\n",
      "Epoch: 08 | Time: 0m 17s\n",
      "\tTrain Loss: 1.862 | Train PPL:   6.436\n",
      "\t Val. Loss: 1.978 |  Val. PPL:   7.225\n",
      "Epoch: 09 | Time: 0m 17s\n",
      "\tTrain Loss: 1.670 | Train PPL:   5.314\n",
      "\t Val. Loss: 1.889 |  Val. PPL:   6.615\n",
      "Epoch: 10 | Time: 0m 17s\n",
      "\tTrain Loss: 1.514 | Train PPL:   4.545\n",
      "\t Val. Loss: 1.857 |  Val. PPL:   6.404\n",
      "Epoch: 11 | Time: 0m 17s\n",
      "\tTrain Loss: 1.386 | Train PPL:   3.997\n",
      "\t Val. Loss: 1.820 |  Val. PPL:   6.173\n",
      "Epoch: 12 | Time: 0m 17s\n",
      "\tTrain Loss: 1.280 | Train PPL:   3.595\n",
      "\t Val. Loss: 1.811 |  Val. PPL:   6.119\n",
      "Epoch: 13 | Time: 0m 18s\n",
      "\tTrain Loss: 1.183 | Train PPL:   3.266\n",
      "\t Val. Loss: 1.829 |  Val. PPL:   6.226\n",
      "Epoch: 14 | Time: 0m 18s\n",
      "\tTrain Loss: 1.098 | Train PPL:   2.998\n",
      "\t Val. Loss: 1.828 |  Val. PPL:   6.221\n",
      "Epoch: 15 | Time: 0m 17s\n",
      "\tTrain Loss: 1.025 | Train PPL:   2.787\n",
      "\t Val. Loss: 1.844 |  Val. PPL:   6.325\n",
      "Epoch: 16 | Time: 0m 17s\n",
      "\tTrain Loss: 0.962 | Train PPL:   2.616\n",
      "\t Val. Loss: 1.867 |  Val. PPL:   6.468\n",
      "Epoch: 17 | Time: 0m 17s\n",
      "\tTrain Loss: 0.906 | Train PPL:   2.474\n",
      "\t Val. Loss: 1.881 |  Val. PPL:   6.557\n",
      "Epoch: 18 | Time: 0m 17s\n",
      "\tTrain Loss: 0.858 | Train PPL:   2.358\n",
      "\t Val. Loss: 1.930 |  Val. PPL:   6.889\n",
      "Epoch: 19 | Time: 0m 17s\n",
      "\tTrain Loss: 0.811 | Train PPL:   2.250\n",
      "\t Val. Loss: 1.932 |  Val. PPL:   6.903\n",
      "Epoch: 20 | Time: 0m 17s\n",
      "\tTrain Loss: 0.772 | Train PPL:   2.164\n",
      "\t Val. Loss: 1.988 |  Val. PPL:   7.299\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utilities.TrainModel import TrainModel\n",
    "\n",
    "train_Transformer = TrainModel(model=model3,\n",
    "                         train_iterator=train_iterator,\n",
    "                         val_iterator=valid_iterator,\n",
    "                         optimizer=optimizer,\n",
    "                         criterion=criterion,\n",
    "                         model_type='Transformer')\n",
    "\n",
    "N_EPOCHS = 20\n",
    "CLIP = 1\n",
    "train_Transformer.epoch(n_epochs=N_EPOCHS, clip=CLIP, model_name='transformer-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
