{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization Cell\n",
    "data_en = 'C:\\\\Users\\\\under\\\\Datasets\\\\Neural-Machine-Translation\\\\Europarl_fr-en\\\\europarl-v7.fr-en.en'\n",
    "data_fr = 'C:\\\\Users\\\\under\\\\Datasets\\\\Neural-Machine-Translation\\\\Europarl_fr-en\\\\europarl-v7.fr-en.fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MyTranslationDataset as mtd\n",
    "import random\n",
    "import io, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, trg = mtd.readFiles(src_path=data_fr, trg_path=data_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics on Europarl-v7.fr-en shows that sentence lengths of both languages have correlation. So, this program splits pairs by length of one language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick=50\n",
    "data = {}\n",
    "for src_sen, trg_sen in zip(src, trg):\n",
    "    sentence_range = int(len(src_sen.split())/tick)\n",
    "    # check if sentence_range is OOD (Out of Dictionary)\n",
    "    # if the number already exists in data, append the pair to list\n",
    "    if sentence_range in data:\n",
    "        data[sentence_range].append((src_sen, trg_sen))\n",
    "    # otherwise, add new element and list associated with it\n",
    "    else:\n",
    "        data.update({sentence_range:[(src_sen, trg_sen)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 0: 0-49 words, # projects = 1847824\n",
      "Key 1: 50-99 words, # projects = 155459\n",
      "Key 2: 100-149 words, # projects = 4033\n",
      "Key 3: 150-199 words, # projects = 322\n",
      "Key 4: 200-249 words, # projects = 56\n",
      "Key 5: 250-299 words, # projects = 11\n",
      "Key 6: 300-349 words, # projects = 7\n",
      "Key 7: 350-399 words, # projects = 5\n",
      "Key 8: 400-449 words, # projects = 1\n",
      "Key 9: 450-499 words, # projects = 4\n",
      "Key 13: 650-699 words, # projects = 1\n"
     ]
    }
   ],
   "source": [
    "for key, value in sorted(data.items(), key=lambda x: x[0]):\n",
    "#     print(f'{key*tick}-{key*tick+tick-1} - {len(value[0][0].split())}')\n",
    "#     print(f'{key*tick}-{key*tick+tick-1} - {value[0][1].split()}')\n",
    "    print(f'Key {key}: {key*tick}-{key*tick+tick-1} words, # projects = {len(value)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = data.get(2) + data.get(3) + data.get(4) + \\\n",
    "         data.get(5) + data.get(6) + data.get(7) + data.get(8) + \\\n",
    "         data.get(9) + data.get(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49440"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master = master + random.sample(data.get(0), 22500) + random.sample(data.get(1), 22500)\n",
    "len(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In my opinion, this second hypothesis would imply the failure of Parliament in its duty as a Parliament, as well as introducing an original thesis, an unknown method which consists of making political groups aware, in writing, of a speech concerning the Commission' s programme a week earlier - and not a day earlier, as had been agreed - bearing in mind that the legislative programme will be discussed in February, so we could forego the debate, since on the next day our citizens will hear about it in the press and on the Internet and Parliament will no longer have to worry about it.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataset\n",
    "# Name is test.fr or test.en to be used in fairseq\n",
    "export_path = 'C:\\\\Users\\\\under\\\\Datasets\\\\Neural-Machine-Translation\\\\Europarl_fr-en_balanced'\n",
    "os.mkdir(export_path)\n",
    "\n",
    "with io.open(os.path.join(export_path,'test'+'.fr'), mode='w', encoding='utf-8') as f_src, \\\n",
    "     io.open(os.path.join(export_path,'test'+'.en'), mode='w', encoding='utf-8') as f_trg:\n",
    "    # src file\n",
    "    f_src.write(' \\n '.join([d[0] for d in master]))\n",
    "    # trg file\n",
    "    f_trg.write(' \\n '.join([d[1] for d in master]))\n",
    "    # close files\n",
    "    f_src.close()\n",
    "    f_trg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
